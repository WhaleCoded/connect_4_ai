{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import sklearn\n",
    "\n",
    "BOARD_WIDTH = 7\n",
    "BOARD_HEIGHT = 6\n",
    "MAX_BOARDS = 1_000_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_moves_to_game_board(moves):\n",
    "    moves_by_column = {}\n",
    "\n",
    "    curr_player = 1\n",
    "    for move in moves:\n",
    "        col_index = int(move) - 1\n",
    "\n",
    "        if col_index not in moves_by_column:\n",
    "            moves_by_column[col_index] = []\n",
    "\n",
    "        moves_by_column[col_index].append(curr_player)\n",
    "\n",
    "        if curr_player == 1:\n",
    "            curr_player = -1\n",
    "        else:\n",
    "            curr_player = 1\n",
    "\n",
    "    game_board = np.zeros((BOARD_WIDTH, BOARD_HEIGHT), dtype=np.int8)\n",
    "    for col_index, moves in moves_by_column.items():\n",
    "        for row_index, move in enumerate(moves):\n",
    "            game_board[col_index][row_index] = move\n",
    "\n",
    "    return game_board\n",
    "\n",
    "def get_col_scores(entries):\n",
    "    col_scores = np.zeros(BOARD_WIDTH)\n",
    "\n",
    "    for col_index, col_score in enumerate(entries):\n",
    "        col_scores[col_index] = int(col_score)\n",
    "\n",
    "    return col_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting lines\n",
      "Randomly selecting game boards\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Converting moves to game boards:   0%|          | 155/206062531 [00:36<13491:25:37,  4.24it/s]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    }
   ],
   "source": [
    "print(\"Counting lines\")\n",
    "num_lines = sum(1 for _ in open(\"answers20.txt\"))\n",
    "available_game_boards = np.array([i for i in range(num_lines)])\n",
    "\n",
    "print(\"Randomly selecting game boards\")\n",
    "selected_game_boards = set(np.random.choice(available_game_boards, size=MAX_BOARDS, replace=False))\n",
    "del available_game_boards\n",
    "\n",
    "game_boards = np.zeros((MAX_BOARDS, BOARD_WIDTH*BOARD_HEIGHT), dtype=np.int8)\n",
    "game_results = np.zeros((MAX_BOARDS, BOARD_WIDTH), dtype=np.int8)\n",
    "with open(\"answers20.txt\", \"r\") as f:\n",
    "    curr_row = 0\n",
    "    board_index = 0\n",
    "\n",
    "    pbar = tqdm(total=num_lines, desc=\"Converting moves to game boards\")\n",
    "    for row in f:\n",
    "        if curr_row in selected_game_boards:\n",
    "            entries = row.split(\" \")\n",
    "\n",
    "            game_board = convert_moves_to_game_board(entries[0])\n",
    "\n",
    "            # if check_for_valid_game_board(game_board, 1) and check_for_valid_game_board(game_board, 2):\n",
    "            game_boards[board_index] = game_board.flatten()\n",
    "\n",
    "            col_scores = get_col_scores(entries[1:])\n",
    "            max_value = np.max(col_scores)\n",
    "            game_results[board_index] = (col_scores == max_value)\n",
    "\n",
    "            board_index += 1\n",
    "\n",
    "        curr_row += 1\n",
    "        pbar.update(1)\n",
    "\n",
    "print(len(game_boards))\n",
    "del selected_game_boards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(game_boards, game_results, test_size=0.1, random_state=None, shuffle=True, stratify=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 3.03640774\n",
      "Iteration 2, loss = 2.70557464\n",
      "Iteration 3, loss = 2.56826676\n",
      "Iteration 4, loss = 2.48707838\n",
      "Iteration 5, loss = 2.43626237\n",
      "Iteration 6, loss = 2.40005626\n",
      "Iteration 7, loss = 2.37184588\n",
      "Iteration 8, loss = 2.34730273\n",
      "Iteration 9, loss = 2.32566757\n",
      "Iteration 10, loss = 2.30724327\n",
      "Iteration 11, loss = 2.29153902\n",
      "Iteration 12, loss = 2.27763081\n",
      "Iteration 13, loss = 2.26607393\n",
      "Iteration 14, loss = 2.25610243\n",
      "Iteration 15, loss = 2.24680691\n",
      "Iteration 16, loss = 2.23908828\n",
      "Iteration 17, loss = 2.23106767\n",
      "Iteration 18, loss = 2.22507853\n",
      "Iteration 19, loss = 2.21890813\n",
      "Iteration 20, loss = 2.21222609\n",
      "Iteration 21, loss = 2.20538505\n",
      "Iteration 22, loss = 2.19862779\n",
      "Iteration 23, loss = 2.19309882\n",
      "Iteration 24, loss = 2.18836508\n",
      "Iteration 25, loss = 2.18348875\n",
      "Iteration 26, loss = 2.17945846\n",
      "Iteration 27, loss = 2.17645668\n",
      "Iteration 28, loss = 2.17199201\n",
      "Iteration 29, loss = 2.16900682\n",
      "Iteration 30, loss = 2.16487452\n",
      "Iteration 31, loss = 2.16161746\n",
      "Iteration 32, loss = 2.15873340\n",
      "Iteration 33, loss = 2.15440506\n",
      "Iteration 34, loss = 2.15199292\n",
      "Iteration 35, loss = 2.14923015\n",
      "Iteration 36, loss = 2.14627599\n",
      "Iteration 37, loss = 2.14397375\n",
      "Iteration 38, loss = 2.14128260\n",
      "Iteration 39, loss = 2.13855785\n",
      "Iteration 40, loss = 2.13600441\n",
      "Iteration 41, loss = 2.13387499\n",
      "Iteration 42, loss = 2.13053081\n",
      "Iteration 43, loss = 2.12906629\n",
      "Iteration 44, loss = 2.12717867\n",
      "Iteration 45, loss = 2.12574605\n",
      "Iteration 46, loss = 2.12359577\n",
      "Iteration 47, loss = 2.12210819\n",
      "Iteration 48, loss = 2.12043428\n",
      "Iteration 49, loss = 2.11941807\n",
      "Iteration 50, loss = 2.11820089\n",
      "Iteration 51, loss = 2.11677587\n",
      "Iteration 52, loss = 2.11532458\n",
      "Iteration 53, loss = 2.11329294\n",
      "Iteration 54, loss = 2.11247721\n",
      "Iteration 55, loss = 2.11105389\n",
      "Iteration 56, loss = 2.10983620\n",
      "Iteration 57, loss = 2.10858121\n",
      "Iteration 58, loss = 2.10797813\n",
      "Iteration 59, loss = 2.10706451\n",
      "Iteration 60, loss = 2.10614266\n",
      "Iteration 61, loss = 2.10501804\n",
      "Iteration 62, loss = 2.10346459\n",
      "Iteration 63, loss = 2.10204514\n",
      "Iteration 64, loss = 2.10172052\n",
      "Iteration 65, loss = 2.10038973\n",
      "Iteration 66, loss = 2.10060108\n",
      "Iteration 67, loss = 2.09923094\n",
      "Iteration 68, loss = 2.09734827\n",
      "Iteration 69, loss = 2.09730507\n",
      "Iteration 70, loss = 2.09566171\n",
      "Iteration 71, loss = 2.09531832\n",
      "Iteration 72, loss = 2.09561684\n",
      "Iteration 73, loss = 2.09349398\n",
      "Iteration 74, loss = 2.09287335\n",
      "Iteration 75, loss = 2.09169000\n",
      "Iteration 76, loss = 2.09134813\n",
      "Iteration 77, loss = 2.09011544\n",
      "Iteration 78, loss = 2.09013741\n",
      "Iteration 79, loss = 2.08913177\n",
      "Iteration 80, loss = 2.08837077\n",
      "Iteration 81, loss = 2.08853679\n",
      "Iteration 82, loss = 2.08628981\n",
      "Iteration 83, loss = 2.08595985\n",
      "Iteration 84, loss = 2.08563678\n",
      "Iteration 85, loss = 2.08422278\n",
      "Iteration 86, loss = 2.08268382\n",
      "Iteration 87, loss = 2.08196749\n",
      "Iteration 88, loss = 2.08102509\n",
      "Iteration 89, loss = 2.07986394\n",
      "Iteration 90, loss = 2.07889398\n",
      "Iteration 91, loss = 2.07779783\n",
      "Iteration 92, loss = 2.07823634\n",
      "Iteration 93, loss = 2.07703947\n",
      "Iteration 94, loss = 2.07609221\n",
      "Iteration 95, loss = 2.07562497\n",
      "Iteration 96, loss = 2.07520211\n",
      "Iteration 97, loss = 2.07407975\n",
      "Iteration 98, loss = 2.07307904\n",
      "Iteration 99, loss = 2.07316610\n",
      "Iteration 100, loss = 2.07190272\n",
      "Iteration 101, loss = 2.07205280\n",
      "Iteration 102, loss = 2.07152903\n",
      "Iteration 103, loss = 2.07141350\n",
      "Iteration 104, loss = 2.07054528\n",
      "Iteration 105, loss = 2.07000644\n",
      "Iteration 106, loss = 2.06990294\n",
      "Iteration 107, loss = 2.06932254\n",
      "Iteration 108, loss = 2.06843792\n",
      "Iteration 109, loss = 2.06876504\n",
      "Iteration 110, loss = 2.06791835\n",
      "Iteration 111, loss = 2.06755620\n",
      "Iteration 112, loss = 2.06695464\n",
      "Iteration 113, loss = 2.06764595\n",
      "Iteration 114, loss = 2.06657250\n",
      "Iteration 115, loss = 2.06595440\n",
      "Iteration 116, loss = 2.06547011\n",
      "Iteration 117, loss = 2.06503487\n",
      "Iteration 118, loss = 2.06482290\n",
      "Iteration 119, loss = 2.06421655\n",
      "Iteration 120, loss = 2.06452448\n",
      "Iteration 121, loss = 2.06352512\n",
      "Iteration 122, loss = 2.06279916\n",
      "Iteration 123, loss = 2.06320830\n",
      "Iteration 124, loss = 2.06253952\n",
      "Iteration 125, loss = 2.06167254\n",
      "Iteration 126, loss = 2.06096721\n",
      "Iteration 127, loss = 2.06137733\n",
      "Iteration 128, loss = 2.06074312\n",
      "Iteration 129, loss = 2.06082987\n",
      "Iteration 130, loss = 2.05944893\n",
      "Iteration 131, loss = 2.05951676\n",
      "Iteration 132, loss = 2.05915142\n",
      "Iteration 133, loss = 2.05870056\n",
      "Iteration 134, loss = 2.05840397\n",
      "Iteration 135, loss = 2.05849808\n",
      "Iteration 136, loss = 2.05766364\n",
      "Iteration 137, loss = 2.05816282\n",
      "Iteration 138, loss = 2.05751519\n",
      "Iteration 139, loss = 2.05641605\n",
      "Iteration 140, loss = 2.05673283\n",
      "Iteration 141, loss = 2.05631329\n",
      "Iteration 142, loss = 2.05648665\n",
      "Iteration 143, loss = 2.05559609\n",
      "Iteration 144, loss = 2.05536351\n",
      "Iteration 145, loss = 2.05477041\n",
      "Iteration 146, loss = 2.05427016\n",
      "Iteration 147, loss = 2.05469937\n",
      "Iteration 148, loss = 2.05383495\n",
      "Iteration 149, loss = 2.05383494\n",
      "Iteration 150, loss = 2.05286958\n",
      "Iteration 151, loss = 2.05306944\n",
      "Iteration 152, loss = 2.05302886\n",
      "Iteration 153, loss = 2.05170642\n",
      "Iteration 154, loss = 2.05217400\n",
      "Iteration 155, loss = 2.05157514\n",
      "Iteration 156, loss = 2.05160541\n",
      "Iteration 157, loss = 2.05122278\n",
      "Iteration 158, loss = 2.05064065\n",
      "Iteration 159, loss = 2.05073280\n",
      "Iteration 160, loss = 2.05047244\n",
      "Iteration 161, loss = 2.05113799\n",
      "Iteration 162, loss = 2.04971623\n",
      "Iteration 163, loss = 2.05016094\n",
      "Iteration 164, loss = 2.04985679\n",
      "Iteration 165, loss = 2.04938231\n",
      "Iteration 166, loss = 2.04867716\n",
      "Iteration 167, loss = 2.04913718\n",
      "Iteration 168, loss = 2.04817256\n",
      "Iteration 169, loss = 2.04839920\n",
      "Iteration 170, loss = 2.04778026\n",
      "Iteration 171, loss = 2.04703639\n",
      "Iteration 172, loss = 2.04681458\n",
      "Iteration 173, loss = 2.04765386\n",
      "Iteration 174, loss = 2.04696889\n",
      "Iteration 175, loss = 2.04677705\n",
      "Iteration 176, loss = 2.04641840\n",
      "Iteration 177, loss = 2.04593085\n",
      "Iteration 178, loss = 2.04567196\n",
      "Iteration 179, loss = 2.04588604\n",
      "Iteration 180, loss = 2.04476584\n",
      "Iteration 181, loss = 2.04508333\n",
      "Iteration 182, loss = 2.04282625\n",
      "Iteration 183, loss = 2.04317475\n",
      "Iteration 184, loss = 2.04312813\n",
      "Iteration 185, loss = 2.04197800\n",
      "Iteration 186, loss = 2.04208998\n",
      "Iteration 187, loss = 2.04228418\n",
      "Iteration 188, loss = 2.04124438\n",
      "Iteration 189, loss = 2.04123990\n",
      "Iteration 190, loss = 2.04121312\n",
      "Iteration 191, loss = 2.04046830\n",
      "Iteration 192, loss = 2.04013866\n",
      "Iteration 193, loss = 2.04059220\n",
      "Iteration 194, loss = 2.03968200\n",
      "Iteration 195, loss = 2.03954879\n",
      "Iteration 196, loss = 2.03960079\n",
      "Iteration 197, loss = 2.03876860\n",
      "Iteration 198, loss = 2.03958254\n",
      "Iteration 199, loss = 2.03866052\n",
      "Iteration 200, loss = 2.03919711\n",
      "Iteration 201, loss = 2.03828283\n",
      "Iteration 202, loss = 2.03790919\n",
      "Iteration 203, loss = 2.03850854\n",
      "Iteration 204, loss = 2.03800698\n",
      "Iteration 205, loss = 2.03824566\n",
      "Iteration 206, loss = 2.03707525\n",
      "Iteration 207, loss = 2.03763626\n",
      "Iteration 208, loss = 2.03784232\n",
      "Iteration 209, loss = 2.03699215\n",
      "Iteration 210, loss = 2.03627581\n",
      "Iteration 211, loss = 2.03647064\n",
      "Iteration 212, loss = 2.03601800\n",
      "Iteration 213, loss = 2.03603476\n",
      "Iteration 214, loss = 2.03606651\n",
      "Iteration 215, loss = 2.03620513\n",
      "Iteration 216, loss = 2.03597536\n",
      "0.54388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dallin/miniconda3/envs/experiments/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:699: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(300), max_iter=1000, verbose=True)\n",
    "\n",
    "\n",
    "mlp.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "score = mlp.score(x_test, y_test)\n",
    "\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 3.14006574\n",
      "Iteration 2, loss = 2.75246016\n",
      "Iteration 3, loss = 2.58535476\n",
      "Iteration 4, loss = 2.48144315\n",
      "Iteration 5, loss = 2.41330134\n",
      "Iteration 6, loss = 2.36395930\n",
      "Iteration 7, loss = 2.32906113\n",
      "Iteration 8, loss = 2.29951904\n",
      "Iteration 9, loss = 2.27517511\n",
      "Iteration 10, loss = 2.25668824\n",
      "Iteration 11, loss = 2.23763123\n",
      "Iteration 12, loss = 2.22341977\n",
      "Iteration 13, loss = 2.20984503\n",
      "Iteration 14, loss = 2.19760248\n",
      "Iteration 15, loss = 2.18457843\n",
      "Iteration 16, loss = 2.17492245\n",
      "Iteration 17, loss = 2.16464737\n",
      "Iteration 18, loss = 2.15491571\n",
      "Iteration 19, loss = 2.14649380\n",
      "Iteration 20, loss = 2.14082320\n",
      "0.5408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dallin/miniconda3/envs/experiments/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(50, 50, 50), max_iter=20, verbose=True)\n",
    "\n",
    "\n",
    "mlp.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "score = mlp.score(x_test, y_test)\n",
    "\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.60914568\n",
      "Iteration 2, loss = 1.93108754\n",
      "Iteration 3, loss = 1.68528914\n",
      "Iteration 4, loss = 1.53359119\n",
      "Iteration 5, loss = 1.42022101\n",
      "Iteration 6, loss = 1.33050785\n",
      "Iteration 7, loss = 1.25879541\n",
      "Iteration 8, loss = 1.20016047\n",
      "Iteration 9, loss = 1.15392479\n",
      "Iteration 10, loss = 1.11462627\n",
      "Iteration 11, loss = 1.08410946\n",
      "Iteration 12, loss = 1.05888584\n",
      "Iteration 13, loss = 1.03742907\n",
      "Iteration 14, loss = 1.02061562\n",
      "Iteration 15, loss = 1.00335795\n",
      "Iteration 16, loss = 0.98979145\n",
      "Iteration 17, loss = 0.97749015\n",
      "Iteration 18, loss = 0.96678260\n",
      "Iteration 19, loss = 0.95511027\n",
      "Iteration 20, loss = 0.94894566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dallin/miniconda3/envs/experiments/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75973\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(200, 200, 200), max_iter=20, verbose=True)\n",
    "\n",
    "\n",
    "mlp.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "score = mlp.score(x_test, y_test)\n",
    "\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.41664681\n",
      "Iteration 2, loss = 1.61007784\n",
      "Iteration 3, loss = 1.33997433\n",
      "Iteration 4, loss = 1.17739185\n",
      "Iteration 5, loss = 1.07253540\n",
      "Iteration 6, loss = 0.99947180\n",
      "Iteration 7, loss = 0.94921554\n",
      "Iteration 8, loss = 0.91095280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dallin/miniconda3/envs/experiments/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:699: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.77746\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(400, 400, 400, 200, 100, 50), max_iter=10, verbose=True)\n",
    "\n",
    "\n",
    "mlp.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "score = mlp.score(x_test, y_test)\n",
    "\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26555\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=100, weights='distance')\n",
    "\n",
    "knn = knn.fit(x_train, y_train)\n",
    "\n",
    "score = knn.score(x_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "y should be a 1d array, got an array of shape (900000, 7) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [15], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msvm\u001b[39;00m \u001b[39mimport\u001b[39;00m SVC\n\u001b[1;32m      3\u001b[0m svm \u001b[39m=\u001b[39m SVC(kernel\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrbf\u001b[39m\u001b[39m'\u001b[39m, C\u001b[39m=\u001b[39m\u001b[39m1.0\u001b[39m, gamma\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m'\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m----> 5\u001b[0m svm \u001b[39m=\u001b[39m svm\u001b[39m.\u001b[39;49mfit(x_train, y_train)\n\u001b[1;32m      7\u001b[0m score \u001b[39m=\u001b[39m svm\u001b[39m.\u001b[39mscore(x_test, y_test)\n\u001b[1;32m      8\u001b[0m \u001b[39mprint\u001b[39m(score)\n",
      "File \u001b[0;32m~/miniconda3/envs/experiments/lib/python3.9/site-packages/sklearn/svm/_base.py:190\u001b[0m, in \u001b[0;36mBaseLibSVM.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    188\u001b[0m     check_consistent_length(X, y)\n\u001b[1;32m    189\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m     X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[1;32m    191\u001b[0m         X,\n\u001b[1;32m    192\u001b[0m         y,\n\u001b[1;32m    193\u001b[0m         dtype\u001b[39m=\u001b[39;49mnp\u001b[39m.\u001b[39;49mfloat64,\n\u001b[1;32m    194\u001b[0m         order\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mC\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    195\u001b[0m         accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    196\u001b[0m         accept_large_sparse\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    197\u001b[0m     )\n\u001b[1;32m    199\u001b[0m y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_targets(y)\n\u001b[1;32m    201\u001b[0m sample_weight \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(\n\u001b[1;32m    202\u001b[0m     [] \u001b[39mif\u001b[39;00m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m sample_weight, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat64\n\u001b[1;32m    203\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/experiments/lib/python3.9/site-packages/sklearn/base.py:581\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    579\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[1;32m    580\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 581\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[1;32m    582\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[1;32m    584\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/miniconda3/envs/experiments/lib/python3.9/site-packages/sklearn/utils/validation.py:979\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m    962\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39my cannot be None\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    964\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[1;32m    965\u001b[0m     X,\n\u001b[1;32m    966\u001b[0m     accept_sparse\u001b[39m=\u001b[39maccept_sparse,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    976\u001b[0m     estimator\u001b[39m=\u001b[39mestimator,\n\u001b[1;32m    977\u001b[0m )\n\u001b[0;32m--> 979\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39;49mmulti_output, y_numeric\u001b[39m=\u001b[39;49my_numeric)\n\u001b[1;32m    981\u001b[0m check_consistent_length(X, y)\n\u001b[1;32m    983\u001b[0m \u001b[39mreturn\u001b[39;00m X, y\n",
      "File \u001b[0;32m~/miniconda3/envs/experiments/lib/python3.9/site-packages/sklearn/utils/validation.py:993\u001b[0m, in \u001b[0;36m_check_y\u001b[0;34m(y, multi_output, y_numeric)\u001b[0m\n\u001b[1;32m    989\u001b[0m     y \u001b[39m=\u001b[39m check_array(\n\u001b[1;32m    990\u001b[0m         y, accept_sparse\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcsr\u001b[39m\u001b[39m\"\u001b[39m, force_all_finite\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, ensure_2d\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m\n\u001b[1;32m    991\u001b[0m     )\n\u001b[1;32m    992\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 993\u001b[0m     y \u001b[39m=\u001b[39m column_or_1d(y, warn\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    994\u001b[0m     _assert_all_finite(y)\n\u001b[1;32m    995\u001b[0m     _ensure_no_complex_data(y)\n",
      "File \u001b[0;32m~/miniconda3/envs/experiments/lib/python3.9/site-packages/sklearn/utils/validation.py:1038\u001b[0m, in \u001b[0;36mcolumn_or_1d\u001b[0;34m(y, warn)\u001b[0m\n\u001b[1;32m   1029\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   1030\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mA column-vector y was passed when a 1d array was\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1031\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m expected. Please change the shape of y to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m             stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[1;32m   1035\u001b[0m         )\n\u001b[1;32m   1036\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mravel(y)\n\u001b[0;32m-> 1038\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1039\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39my should be a 1d array, got an array of shape \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m instead.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(shape)\n\u001b[1;32m   1040\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: y should be a 1d array, got an array of shape (900000, 7) instead."
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC(kernel='rbf', C=1.0, gamma='auto', verbose=True)\n",
    "\n",
    "svm = svm.fit(x_train, y_train)\n",
    "\n",
    "score = svm.score(x_test, y_test)\n",
    "print(score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('experiments')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c7ebe76d8d969ec5a1e9669fe2b882b7ab37dfff00812bf23b1b4f280354a721"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
